{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda9d65e615e5564808b0fd10cb7293bc83",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x,y,z,w,b是否为叶子节点:True,False,False,True,True\nx,w, b 的属性为False,True,True\ny, z 的 require_grad属性：True,True\nx, w, b grad_fn is None,None,None\ny, z grad_fn is <MulBackward0 object at 0x000001F057F98708>,<AddBackward0 object at 0x000001F057F98188>\nw, b grad istensor([2.]),tensor([1.])\ny, z grad is None, None\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 标量的反向传播\n",
    "x = torch.Tensor([2])\n",
    "\n",
    "# autograd: set require_grad = True\n",
    "w = torch.randn(1, requires_grad= True)\n",
    "b = torch.randn(1, requires_grad= True)\n",
    "\n",
    "# 前向传播，计算图，分步计算\n",
    "y = torch.mul(x, w)\n",
    "z = torch.add(y, b)\n",
    "\n",
    "# 查看是否为is_leaf叶子节点\n",
    "print('x,y,z,w,b是否为叶子节点:{},{},{},{},{}'.format(x.is_leaf, y.is_leaf, z.is_leaf, w.is_leaf, b.is_leaf))\n",
    "\n",
    "# 查看子节点x, w, b的requires_grad属性\n",
    "print('x,w, b 的属性为{},{},{}'.format(x.requires_grad, w.requires_grad, b.requires_grad))\n",
    "\n",
    "# 查看非叶子节点的requires_grad梯度属性\n",
    "print('y, z 的 require_grad属性：{},{}'.format(y.requires_grad, z.requires_grad)) #与w, b有依赖关系\n",
    "\n",
    "# 查看叶子节点的grad_fn属性\n",
    "print('x, w, b grad_fn is {},{},{}'.format(x.grad_fn, w.grad_fn, b.grad_fn))\n",
    "\n",
    "# 查看非叶子节点的grad_fn属性\n",
    "print('y, z grad_fn is {},{}'.format(y.grad_fn, z.grad_fn))\n",
    "\n",
    "# 反向传播计算梯度\n",
    "# z.backward() #此时不保留图graph，梯度清零\n",
    "z.backward(retain_graph = True) # 梯度累加 ？\n",
    "\n",
    "# 叶子节点：参数w b梯度, x无需求导 故为none\n",
    "print('w, b grad is{},{}'.format(w.grad, b.grad))\n",
    "# 非叶子节点 y z 梯度\n",
    "print('y, z grad is {}, {}'.format(y.grad, z.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "参数w,b的梯度为:tensor([4.]),tensor([2.])\n"
    }
   ],
   "source": [
    "# 返回，自动求导\n",
    "# z.backward(retain_graph= True) 保留图，否则运行一次缓存被释放\n",
    "z.backward()\n",
    "# 看梯度\n",
    "print('参数w,b的梯度为:{},{}'.format(w.grad, b.grad))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}