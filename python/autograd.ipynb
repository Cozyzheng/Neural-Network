{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x,y,z,w,b是否为叶子节点:True,False,False,True,True\nx,w, b 的属性为False,True,True\ny, z 的 require_grad属性：True,True\nx, w, b grad_fn is None,None,None\ny, z grad_fn is <MulBackward0 object at 0x111b16e50>,<AddBackward0 object at 0x111b16ed0>\nw, b grad istensor([2.]),tensor([1.])\ny, z grad is None, None\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 标量的反向传播\n",
    "x = torch.Tensor([2])\n",
    "\n",
    "# autograd: set require_grad = True\n",
    "w = torch.randn(1, requires_grad= True)\n",
    "b = torch.randn(1, requires_grad= True)\n",
    "\n",
    "# 前向传播，计算图，分步计算\n",
    "y = torch.mul(x, w)\n",
    "z = torch.add(y, b)\n",
    "\n",
    "# 查看是否为is_leaf叶子节点\n",
    "print('x,y,z,w,b是否为叶子节点:{},{},{},{},{}'.format(x.is_leaf, y.is_leaf, z.is_leaf, w.is_leaf, b.is_leaf))\n",
    "\n",
    "# 查看子节点x, w, b的requires_grad属性\n",
    "print('x,w, b 的属性为{},{},{}'.format(x.requires_grad, w.requires_grad, b.requires_grad))\n",
    "\n",
    "# 查看非叶子节点的requires_grad梯度属性\n",
    "print('y, z 的 require_grad属性：{},{}'.format(y.requires_grad, z.requires_grad)) #与w, b有依赖关系\n",
    "\n",
    "# 查看叶子节点的grad_fn属性\n",
    "print('x, w, b grad_fn is {},{},{}'.format(x.grad_fn, w.grad_fn, b.grad_fn))\n",
    "\n",
    "# 查看非叶子节点的grad_fn属性\n",
    "print('y, z grad_fn is {},{}'.format(y.grad_fn, z.grad_fn))\n",
    "\n",
    "# 反向传播计算梯度\n",
    "# z.backward() #此时不保留图graph，梯度清零\n",
    "z.backward(retain_graph = True) # 梯度累加 ？\n",
    "\n",
    "# 叶子节点：参数w b梯度, x无需求导 故为none\n",
    "print('w, b grad is{},{}'.format(w.grad, b.grad))\n",
    "# 非叶子节点 y z 梯度\n",
    "print('y, z grad is {}, {}'.format(y.grad, z.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17d4fb8e254d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 返回，自动求导\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# z.backward(retain_graph= True) 保留图，否则运行一次缓存被释放\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# 看梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'参数w,b的梯度为:{},{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# 返回，自动求导\n",
    "# z.backward(retain_graph= True) 保留图，否则运行一次缓存被释放\n",
    "z.backward()\n",
    "# 看梯度\n",
    "print('参数w,b的梯度为:{},{}'.format(w.grad, b.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算下面这个函数的导函数：\n",
    "$$\n",
    "y = x^2\\bullet e^x\n",
    "$$\n",
    "它的导函数是：\n",
    "$$\n",
    "{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n",
    "$$\n",
    "来看看autograd的计算结果与手动求导计算结果的误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "grad by torch-autograd: tensor([[-4.4350e-01,  6.9511e+00, -4.1923e-01, -2.3527e-02],\n        [ 1.1022e+00, -1.7225e-01,  3.1054e+00, -2.1411e-01],\n        [ 2.2655e-01,  4.5613e+01,  1.4302e+00,  3.2563e-01]])\ngrad by def: tensor([[-4.4350e-01,  6.9511e+00, -4.1923e-01, -2.3527e-02],\n        [ 1.1022e+00, -1.7225e-01,  3.1054e+00, -2.1411e-01],\n        [ 2.2655e-01,  4.5613e+01,  1.4302e+00,  3.2563e-01]],\n       grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "import torch as t \n",
    "def f(x):\n",
    "    '''计算y'''\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y \n",
    "def gradf(x):\n",
    "    '''手动求导函数'''\n",
    "    dx = 2 * x * t.exp(x) + x ** 2 * t.exp(x)\n",
    "    return dx\n",
    "\n",
    "# 测试一个tensor：x\n",
    "x = t.randn(3,4, requires_grad = True)\n",
    "y = f(x)\n",
    "y.backward(t.ones(y.size()))\n",
    "\n",
    "# 比较两种求导方法\n",
    "print('grad by torch-autograd:',x.grad)\n",
    "print('grad by def:', gradf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算图，测试grad_fn\n",
    "x = t.ones(1)\n",
    "b = t.rand(1, requires_grad = True)\n",
    "w = t.rand(1, requires_grad = True)\n",
    "y = w.mul(x)\n",
    "z = y.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<AddBackward0 object at 0x12fc0b290>\n((<MulBackward0 object at 0x12fc0bb90>, 0), (<AccumulateGrad object at 0x12fc0bc10>, 0))\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([24.])"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "# grad_fn.net_functions,元组的形式，回溯fn，动态图\n",
    "print(z.grad_fn)\n",
    "print(z.grad_fn.next_functions)\n",
    "z.grad_fn.next_functions[0][0] == y.grad_fn\n",
    "\n",
    "# 保存计算图梯度的buffer，多次反向传播，梯度累加\n",
    "z.backward(retain_graph=True)\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False True False\n"
    }
   ],
   "source": [
    "# 测试推理时，不需要反向传播，求导，即可以关闭自动求导autograd，反向传播，节省内存，显存开销\n",
    "t.set_grad_enabled(False)\n",
    "# 或\n",
    "with t.no_grad():\n",
    "    x = t.ones(1)\n",
    "    w = t.rand(1, requires_grad = True)\n",
    "    y = x.mul(w)\n",
    "# 按理说y应该是True，因为叶子节点w是True，但是设置了不使用grad所以为False\n",
    "print(x.requires_grad, w.requires_grad, y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0.0893, 0.1730, 0.1023, 0.2946, 0.5129, 0.2371, 0.1064, 0.9711, 0.8577,\n        0.0347], requires_grad=True)\ntensor([0.0893, 0.1730, 0.1023, 0.2946, 0.5129, 0.2371, 0.1064, 0.9711, 0.8577,\n        0.0347])\ntensor([ 8.9289, 17.2977, 10.2297, 29.4627, 51.2926, 23.7130, 10.6423, 97.1148,\n        85.7733,  3.4686], requires_grad=True)\n"
    }
   ],
   "source": [
    "# 想改变tensor的数据，而不影响grad时\n",
    "x = t.rand(10, requires_grad = True)\n",
    "print(x)\n",
    "# 直接访问tensor的data，而不影响grad，独立计算图之外\n",
    "print(x.data)\n",
    "\n",
    "#改变数据\n",
    "x.data *= 100\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非叶子节点的导数计算完之后即被清空,使用autograd.grad函数,hook函数，查看梯度\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}